# Distributed TensorFlow :hand: fa18-523-68

| Selahattin Akkas
| sakkas@iu.edu
| Indiana University
| hid: fa18-523-68
| github: [:cloud:](https://github.com/cloudmesh-community/fa18-523-68/blob/master/paper/paper.md)

* :o: this is a draft, review has not been started due to this

---

Keywords: Distributed TensorFlow, TensorFlow

---

## Abstract

It is non-practical to do computation on a single machine for Big Data applications. Likewise, it is also non-practical to train machine learning algorithms using large datasets on a single machine. One of the widely used Deep Learning framework TensorFlow supports distributed learning. In this paper, Distributed TensorFlow's architecture will be explained. 

## Introduction

## Parameter Server

## TensorFlow Cluster

## Parameter Server

## Shared Variables

## Synchronus Data Parallelism

## Asynchronus Data Parallelism

## In-graph replication

## Between-graph replication

## Asynchronus training

## Synchronus training

